{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912058c4",
   "metadata": {},
   "source": [
    "Use the starter notebook and write code to help you: \n",
    "1. Specify an ML model that can understand both images and text into Weaviate\n",
    "2. Add the provided data to your vector database - feel free to use your own images for this aswell!\n",
    "3. Search the data with a text query\n",
    "4. Search the data with an image query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610385fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, weaviate, json, os, IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39dad7",
   "metadata": {},
   "source": [
    "### We first need to start a local instance of Weaviate using Docker.\n",
    "\n",
    "1. This can be done by opening up a terminal in the folder with the provided `docker-compose.yml` file in it and typing:\n",
    "```bash\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "2. Later, in order to bring Weaviate down you can just go into this terminal window and type `Ctrl + C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372be9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate is live!\n",
      "Client created:? True\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_local(host=\"localhost\", port=8080, grpc_port=50051, skip_init_checks=True)\n",
    "\n",
    "if client.is_live():\n",
    "    print(\"Weaviate is live!\")\n",
    "else:\n",
    "    print(\"Weaviate is not reachable.\")\n",
    "\n",
    "print(f\"Client created:? {client.is_ready()}\")\n",
    "\n",
    "if client.collections.exists(\"TextImageSearch\"):\n",
    "    client.collections.delete(\"TextImageSearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41fc7d",
   "metadata": {},
   "source": [
    "### Q1: Specify an ML model that can understand both images and text into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c48b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'ClipExample' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#To do this we need to specify a schema in which we can specify the model to be used\n",
    "# aswell as the properties. \n",
    "\n",
    "class_obj = {\n",
    "    \"class\": \"TextImageSearch\",  # use \"name\" instead of \"class\"\n",
    "    \"vectorizer\": \"multi2vec-clip\",\n",
    "    \"moduleConfig\": {\n",
    "        \"multi2vec-clip\": {\"imageFields\": [\"image\"]}\n",
    "    },\n",
    "    \"properties\": [\n",
    "        {\"name\": \"text\", \"dataType\": [\"string\"]},\n",
    "        {\"name\": \"image\", \"dataType\": [\"blob\"]}\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "client.collections.create_from_dict(class_obj)\n",
    "print(\"Collection 'TextImageSearch' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ebdaa18-c578-4a70-83eb-807d4be1dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chhab\\dev\\introduction-to-ai-native-vector-databases-4470531\\03_05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faec5b3",
   "metadata": {},
   "source": [
    "### Q2: Add the provided data to your vector database - feel free to use your own images for this aswell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda39cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images/Big Sur_ California_LIL_9678.jpg\n",
      "Images/Bird feeder_LIL_134172.jpg\n",
      "Images/Blue sky between skyscrapers_LIL_134160.jpg\n",
      "Images/Cats on a chair_LIL_134151.jpg\n",
      "Images/Cat_LIL_134138.jpg\n",
      "Images/Cherry_LIL_134126.jpg\n",
      "Images/Cityscape_LIL_134155.jpg\n",
      "Images/Conservatory_LIL_9680.jpg\n",
      "Images/Deer_LIL_134180.jpg\n",
      "Images/Dog in motion_LIL_134175.jpg\n",
      "Images/Feet under a skirt_LIL_134201.jpg\n",
      "Images/Forest_LIL_134133.jpg\n",
      "Images/Golden Gate Bridge from Presidio_LIL_9682.jpg\n",
      "Images/Joshua Tree and California coast_LIL_9662.jpg\n",
      "Images/Joshua Tree and California coast_LIL_9670.jpg\n",
      "Images/Kitchen scene_LIL_134191.jpg\n",
      "Images/Llama_LIL_134178.jpg\n",
      "Images/Microphone_LIL_134215.jpg\n",
      "Images/On a grassy hill_LIL_134221.jpg\n",
      "Images/Onlookers contemplating_LIL_134225.jpg\n",
      "Images/Ornate furniture_LIL_134195.jpg\n",
      "Images/Pies and dishes_LIL_134217.jpg\n",
      "Images/Point Reyes_ California_LIL_9672.jpg\n",
      "Images/Reading outside_LIL_134129.jpg\n",
      "Images/Single cyclist with land background_LIL_9376.dng\n",
      "Images/Speaker_LIL_134226.jpg\n",
      "Images/Spiral staircase_LIL_134156.jpg\n",
      "Images/Stack of books_LIL_134182.jpg\n",
      "Images/Table setting outdoor_LIL_134165.jpg\n",
      "Images/Walking dog on beach_LIL_134163.jpg\n",
      "Images/Wearing a white dress _LIL_134222.jpg\n",
      "All images added!\n"
     ]
    }
   ],
   "source": [
    "# Here we will pass in a larger dataset into a folder called \"Images\"\n",
    "\n",
    "#If you'd like to add your own images to the vector database to search over\n",
    "# feel free to add them into this folder aswell!\n",
    "clip_collections = client.collections.get(\"TextImageSearch\")\n",
    "for img in os.listdir(\"../03_03/Images/\"):\n",
    "    print(f\"Images/{img}\")\n",
    "\n",
    "    encoded_image = weaviate.util.image_encoder_b64(f\"../03_03/Images/{img}\")\n",
    "    data_properties = {\n",
    "        \"image\": encoded_image,\n",
    "        \"text\": img\n",
    "    }\n",
    "    clip_collections.data.insert(\n",
    "            properties=data_properties\n",
    "            # class_name=\"Question\"\n",
    "    )\n",
    "    \n",
    "print(\"All images added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aa799",
   "metadata": {},
   "source": [
    "### Q3. Search the data with a text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de45f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'text': 'Cats on a chair_LIL_134151.jpg'}\n",
      "0.7432544231414795\n",
      "{'text': 'Cat_LIL_134138.jpg'}\n",
      "0.7630722522735596\n",
      "{'text': 'Bird feeder_LIL_134172.jpg'}\n",
      "0.8153395652770996\n",
      "{'text': 'Spiral staircase_LIL_134156.jpg'}\n",
      "0.8265774250030518\n",
      "{'text': 'On a grassy hill_LIL_134221.jpg'}\n",
      "0.8285894393920898\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import MetadataQuery, Filter\n",
    "\n",
    "print(\"1\")\n",
    "response = clip_collections.query.near_text(query=\"cute cats\", limit=5, return_metadata=MetadataQuery(distance=True))\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.properties)\n",
    "    print(o.metadata.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8a77c",
   "metadata": {},
   "source": [
    "**Visualize some of the returned images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fced12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0d3c4",
   "metadata": {},
   "source": [
    "### Feel free to perform more searches and see if you can explain why specific text search queries return images! \n",
    "\n",
    "#### You've sucessfully used a vector database to build text-to-image search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba56a9",
   "metadata": {},
   "source": [
    "### Q4. Search the data with an image query.\n",
    "\n",
    "- **Here we will pass in images that are not in the vector database and search for the most similar images as determined by vector search** \n",
    "\n",
    "- **These images can be found in the `TestImages` folder. Feel free to provide your own images as search queries here aswell!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616865b",
   "metadata": {},
   "source": [
    "#### First visualize the image you want to query the database with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(filename='TestImages/n03417042_28762.JPEG', width=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247cef6",
   "metadata": {},
   "source": [
    "#### Now let's write a query to search using this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imres = # ADD CODE HERE\n",
    "\n",
    "print(json.dumps(imres,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed284964",
   "metadata": {},
   "source": [
    "**Visualize the images most similar to the input image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
